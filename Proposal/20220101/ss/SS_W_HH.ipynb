{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import Tools.Config as TC\n",
    "import Tools.Func as TF\n",
    "import Node.DS as DS\n",
    "import time\n",
    "import os\n",
    "\n",
    "from pympler import asizeof\n",
    "import re\n",
    "import pandas as pd\n",
    "dataset='webdocs'\n",
    "total_count=0\n",
    "heavy_ratio=1/5000\n",
    "\n",
    "# =============================dataset path and file=============================\n",
    "filepath=r\"..\\..\\dataset\\webdocs\"\n",
    "pattern='out_.*'\n",
    "r=re.compile(pattern)\n",
    "filelist=list(filter(r.match,os.listdir(filepath)))    \n",
    "    # dataset file list\n",
    "\n",
    "gr_file_name='webdocs_00_ground_truth.csv'        \n",
    "gr_path=os.path.join(filepath,gr_file_name)\n",
    "    # ground truth path\n",
    "    \n",
    "# =============================Initialize=============================\n",
    "w=0\n",
    "d=0\n",
    "size=1033\n",
    "topk=1033\n",
    "TC.Set_default(w,d,size,topk)\n",
    "    # set width, depth, size of Sk, random seed of hash\n",
    "    # Config.width, Config.depth\n",
    "Top_dict=dict()\n",
    "    # Top_dict[x]=[count_x,error_x]\n",
    "\n",
    "#item_count=100\n",
    "# =============================Stream processing=============================\n",
    "start=time.time()\n",
    "for datafile in filelist[:1]:\n",
    "    src_data=os.path.join(filepath,datafile)\n",
    "    with open(src_data,'r') as file:\n",
    "        while True:\n",
    "            e=file.readline().strip('\\n')\n",
    "            if not e:\n",
    "                print(\"EOF\")\n",
    "                break\n",
    "            else:\n",
    "                total_count+=1\n",
    "                item=DS.ssNode(e,1)\n",
    "                #item_count-=1\n",
    "                # print(\"read {}th element: {}\".format(item_count,element))\n",
    "                if Top_dict.get(item.ID):\n",
    "                    # e in Top\n",
    "                    Top_dict[item.ID][0]+=item.count\n",
    "                else:\n",
    "                    if len(Top_dict)<TC.size:\n",
    "                        Top_dict[item.ID]=[item.count,item.error]\n",
    "                    else:\n",
    "                        min_ele = min(Top_dict, key=lambda x: Top_dict.get(x)[0])\n",
    "                            # find e_min\n",
    "                        Top_dict[item.ID]=[Top_dict[min_ele][0]+item.count,Top_dict[min_ele][0]]\n",
    "                            # update c_min,c_error\n",
    "                        Top_dict.pop(min_ele)\n",
    "                            # pop old min out\n",
    "end=time.time()\n",
    "\n",
    "# =============================Print and Plot result=============================\n",
    "Top_dict=dict(sorted(Top_dict.items(), key=lambda item: item[1],reverse=True))\n",
    "print(\"Top-{}\".format(TC.size))\n",
    "print(\"Execution time:{:8.3f} seconds.\".format(end-start))\n",
    "\n",
    "# =============================heavy hitter from result=============================\n",
    "HH=dict()\n",
    "for item in Top_dict:\n",
    "    if Top_dict[item][0]>total_count*heavy_ratio:\n",
    "        if (Top_dict[item][0]-Top_dict[item][1])>=total_count*heavy_ratio:\n",
    "            HH[item]=Top_dict[item][0]\n",
    "HH=dict(sorted(HH.items(), key=lambda item: item[1],reverse=True)) \n",
    "\n",
    "# memory usage\n",
    "print(\"Top_dict with {} kbytes.\".format(asizeof.asizeof(Top_dict)/1024))\n",
    "# =============================heavy hitter from ground truth=============================\n",
    "\n",
    "# read ground truth\n",
    "import pandas as pd\n",
    "df=pd.read_csv(gr_path)\n",
    "df['Element']=df['Element'].astype('str')\n",
    "temp=df[df['Count']>=int(total_count*heavy_ratio)]\n",
    "\n",
    "gr_set=set(temp['Element'])\n",
    "result=set(HH.keys())\n",
    "tp_set=gr_set & result\n",
    "\n",
    "print(\"\\nFor copy\")\n",
    "print(\"Top-{}\".format(TC.size))\n",
    "print(\"Top_dict with {} kbytes.\".format(asizeof.asizeof(Top_dict)/1024))\n",
    "print(\"Find {:.3f} of Heavy Hitters\".format(len(tp_set)/len(gr_set)))\n",
    "print(\"Execution time:{:8.3f} seconds.\".format(end-start))\n",
    "\n",
    "# Count ARE/AAE in Top\n",
    "startx=time.time()\n",
    "top_are,top_aae=TF.Get_ARE_AAE(gr_path,HH,tp_set)\n",
    "print(\"Find:{}\".format(len(tp_set)))\n",
    "#print(\"{} item found in SS[{}] compare with true Top-{}\".format(len(tp_set),Config.size,topk))\n",
    "endx=time.time()\n",
    "\n",
    "print(\"Top_ARE: {:6.4f}\".format(top_are))\n",
    "print(\"Top_AAE: {:6.4f}\".format(top_aae))\n",
    "print(\"Estimate time:{:7.3f} seconds.\".format(endx-startx)) \n",
    "\n",
    "TF.Plot_hh_compare(temp,HH,\"SpaceSaving\")\n",
    "\n",
    "# result to csv\n",
    "path=\"..\\\\result\\\\SS\\\\\"+dataset+\"\\\\HH\\\\Top_\"+str(TC.size)\n",
    "filename='SS_HH'+'_'+str(len(tp_set))+'.csv'\n",
    "df=pd.DataFrame(Top_dict.items(),columns=['Element','Count'])\n",
    "df['Element'] = df['Element'].astype(str)\n",
    "df.to_csv(os.path.join(path,filename),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import Tools.Config as TC\n",
    "import Tools.Func as TF\n",
    "import Node.DS as DS\n",
    "import time\n",
    "import os\n",
    "\n",
    "from pympler import asizeof\n",
    "import re\n",
    "import pandas as pd\n",
    "dataset='webdocs'\n",
    "total_count=0\n",
    "heavy_ratio=1/5000\n",
    "\n",
    "# =============================dataset path and file=============================\n",
    "filepath=r\"..\\..\\dataset\\webdocs\"\n",
    "pattern='out_.*'\n",
    "r=re.compile(pattern)\n",
    "filelist=list(filter(r.match,os.listdir(filepath)))    \n",
    "    # dataset file list\n",
    "\n",
    "gr_file_name='webdocs_00_ground_truth.csv'        \n",
    "gr_path=os.path.join(filepath,gr_file_name)\n",
    "    # ground truth path\n",
    "    \n",
    "# =============================Initialize=============================\n",
    "w=0\n",
    "d=0\n",
    "size=1500\n",
    "topk=1500\n",
    "TC.Set_default(w,d,size,topk)\n",
    "    # set width, depth, size of Sk, random seed of hash\n",
    "    # Config.width, Config.depth\n",
    "Top_dict=dict()\n",
    "    # Top_dict[x]=[count_x,error_x]\n",
    "\n",
    "#item_count=100\n",
    "# =============================Stream processing=============================\n",
    "start=time.time()\n",
    "for datafile in filelist[:1]:\n",
    "    src_data=os.path.join(filepath,datafile)\n",
    "    with open(src_data,'r') as file:\n",
    "        while True:\n",
    "            e=file.readline().strip('\\n')\n",
    "            if not e:\n",
    "                print(\"EOF\")\n",
    "                break\n",
    "            else:\n",
    "                total_count+=1\n",
    "                item=DS.ssNode(e,1)\n",
    "                #item_count-=1\n",
    "                # print(\"read {}th element: {}\".format(item_count,element))\n",
    "                if Top_dict.get(item.ID):\n",
    "                    # e in Top\n",
    "                    Top_dict[item.ID][0]+=item.count\n",
    "                else:\n",
    "                    if len(Top_dict)<TC.size:\n",
    "                        Top_dict[item.ID]=[item.count,item.error]\n",
    "                    else:\n",
    "                        min_ele = min(Top_dict, key=lambda x: Top_dict.get(x)[0])\n",
    "                            # find e_min\n",
    "                        Top_dict[item.ID]=[Top_dict[min_ele][0]+item.count,Top_dict[min_ele][0]]\n",
    "                            # update c_min,c_error\n",
    "                        Top_dict.pop(min_ele)\n",
    "                            # pop old min out\n",
    "end=time.time()\n",
    "\n",
    "# =============================Print and Plot result=============================\n",
    "Top_dict=dict(sorted(Top_dict.items(), key=lambda item: item[1],reverse=True))\n",
    "print(\"Top-{}\".format(TC.size))\n",
    "print(\"Execution time:{:8.3f} seconds.\".format(end-start))\n",
    "\n",
    "# =============================heavy hitter from result=============================\n",
    "HH=dict()\n",
    "for item in Top_dict:\n",
    "    if Top_dict[item][0]>total_count*heavy_ratio:\n",
    "        if (Top_dict[item][0]-Top_dict[item][1])>=total_count*heavy_ratio:\n",
    "            HH[item]=Top_dict[item][0]\n",
    "HH=dict(sorted(HH.items(), key=lambda item: item[1],reverse=True)) \n",
    "\n",
    "# memory usage\n",
    "print(\"Top_dict with {} kbytes.\".format(asizeof.asizeof(Top_dict)/1024))\n",
    "# =============================heavy hitter from ground truth=============================\n",
    "\n",
    "# read ground truth\n",
    "import pandas as pd\n",
    "df=pd.read_csv(gr_path)\n",
    "df['Element']=df['Element'].astype('str')\n",
    "temp=df[df['Count']>=int(total_count*heavy_ratio)]\n",
    "\n",
    "gr_set=set(temp['Element'])\n",
    "result=set(HH.keys())\n",
    "tp_set=gr_set & result\n",
    "\n",
    "print(\"\\nFor copy\")\n",
    "print(\"Top-{}\".format(TC.size))\n",
    "print(\"Top_dict with {} kbytes.\".format(asizeof.asizeof(Top_dict)/1024))\n",
    "print(\"Find {:.3f} of Heavy Hitters\".format(len(tp_set)/len(gr_set)))\n",
    "print(\"Execution time:{:8.3f} seconds.\".format(end-start))\n",
    "\n",
    "# Count ARE/AAE in Top\n",
    "startx=time.time()\n",
    "top_are,top_aae=TF.Get_ARE_AAE(gr_path,HH,tp_set)\n",
    "print(\"Find:{}\".format(len(tp_set)))\n",
    "#print(\"{} item found in SS[{}] compare with true Top-{}\".format(len(tp_set),Config.size,topk))\n",
    "endx=time.time()\n",
    "\n",
    "print(\"Top_ARE: {:6.4f}\".format(top_are))\n",
    "print(\"Top_AAE: {:6.4f}\".format(top_aae))\n",
    "print(\"Estimate time:{:7.3f} seconds.\".format(endx-startx)) \n",
    "\n",
    "TF.Plot_hh_compare(temp,HH,\"SpaceSaving\")\n",
    "\n",
    "# result to csv\n",
    "path=\"..\\\\result\\\\SS\\\\\"+dataset+\"\\\\HH\\\\Top_\"+str(TC.size)\n",
    "filename='SS_HH'+'_'+str(len(tp_set))+'.csv'\n",
    "df=pd.DataFrame(Top_dict.items(),columns=['Element','Count'])\n",
    "df['Element'] = df['Element'].astype(str)\n",
    "df.to_csv(os.path.join(path,filename),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import Tools.Config as TC\n",
    "import Tools.Func as TF\n",
    "import Node.DS as DS\n",
    "import time\n",
    "import os\n",
    "\n",
    "from pympler import asizeof\n",
    "import re\n",
    "import pandas as pd\n",
    "dataset='webdocs'\n",
    "total_count=0\n",
    "heavy_ratio=1/5000\n",
    "\n",
    "# =============================dataset path and file=============================\n",
    "filepath=r\"..\\..\\dataset\\webdocs\"\n",
    "pattern='out_.*'\n",
    "r=re.compile(pattern)\n",
    "filelist=list(filter(r.match,os.listdir(filepath)))    \n",
    "    # dataset file list\n",
    "\n",
    "gr_file_name='webdocs_00_ground_truth.csv'        \n",
    "gr_path=os.path.join(filepath,gr_file_name)\n",
    "    # ground truth path\n",
    "    \n",
    "# =============================Initialize=============================\n",
    "w=0\n",
    "d=0\n",
    "size=5000\n",
    "topk=5000\n",
    "TC.Set_default(w,d,size,topk)\n",
    "    # set width, depth, size of Sk, random seed of hash\n",
    "    # Config.width, Config.depth\n",
    "Top_dict=dict()\n",
    "    # Top_dict[x]=[count_x,error_x]\n",
    "\n",
    "#item_count=100\n",
    "# =============================Stream processing=============================\n",
    "start=time.time()\n",
    "for datafile in filelist[:1]:\n",
    "    src_data=os.path.join(filepath,datafile)\n",
    "    with open(src_data,'r') as file:\n",
    "        while True:\n",
    "            e=file.readline().strip('\\n')\n",
    "            if not e:\n",
    "                print(\"EOF\")\n",
    "                break\n",
    "            else:\n",
    "                total_count+=1\n",
    "                item=DS.ssNode(e,1)\n",
    "                #item_count-=1\n",
    "                # print(\"read {}th element: {}\".format(item_count,element))\n",
    "                if Top_dict.get(item.ID):\n",
    "                    # e in Top\n",
    "                    Top_dict[item.ID][0]+=item.count\n",
    "                else:\n",
    "                    if len(Top_dict)<TC.size:\n",
    "                        Top_dict[item.ID]=[item.count,item.error]\n",
    "                    else:\n",
    "                        min_ele = min(Top_dict, key=lambda x: Top_dict.get(x)[0])\n",
    "                            # find e_min\n",
    "                        Top_dict[item.ID]=[Top_dict[min_ele][0]+item.count,Top_dict[min_ele][0]]\n",
    "                            # update c_min,c_error\n",
    "                        Top_dict.pop(min_ele)\n",
    "                            # pop old min out\n",
    "end=time.time()\n",
    "\n",
    "# =============================Print and Plot result=============================\n",
    "Top_dict=dict(sorted(Top_dict.items(), key=lambda item: item[1],reverse=True))\n",
    "print(\"Top-{}\".format(TC.size))\n",
    "print(\"Execution time:{:8.3f} seconds.\".format(end-start))\n",
    "\n",
    "# =============================heavy hitter from result=============================\n",
    "HH=dict()\n",
    "for item in Top_dict:\n",
    "    if Top_dict[item][0]>total_count*heavy_ratio:\n",
    "        if (Top_dict[item][0]-Top_dict[item][1])>=total_count*heavy_ratio:\n",
    "            HH[item]=Top_dict[item][0]\n",
    "HH=dict(sorted(HH.items(), key=lambda item: item[1],reverse=True)) \n",
    "\n",
    "# memory usage\n",
    "print(\"Top_dict with {} kbytes.\".format(asizeof.asizeof(Top_dict)/1024))\n",
    "# =============================heavy hitter from ground truth=============================\n",
    "\n",
    "# read ground truth\n",
    "import pandas as pd\n",
    "df=pd.read_csv(gr_path)\n",
    "df['Element']=df['Element'].astype('str')\n",
    "temp=df[df['Count']>=int(total_count*heavy_ratio)]\n",
    "\n",
    "gr_set=set(temp['Element'])\n",
    "result=set(HH.keys())\n",
    "tp_set=gr_set & result\n",
    "\n",
    "print(\"\\nFor copy\")\n",
    "print(\"Top-{}\".format(TC.size))\n",
    "print(\"Top_dict with {} kbytes.\".format(asizeof.asizeof(Top_dict)/1024))\n",
    "print(\"Find {:.3f} of Heavy Hitters\".format(len(tp_set)/len(gr_set)))\n",
    "print(\"Execution time:{:8.3f} seconds.\".format(end-start))\n",
    "\n",
    "# Count ARE/AAE in Top\n",
    "startx=time.time()\n",
    "top_are,top_aae=TF.Get_ARE_AAE(gr_path,HH,tp_set)\n",
    "print(\"Find:{}\".format(len(tp_set)))\n",
    "#print(\"{} item found in SS[{}] compare with true Top-{}\".format(len(tp_set),Config.size,topk))\n",
    "endx=time.time()\n",
    "\n",
    "print(\"Top_ARE: {:6.4f}\".format(top_are))\n",
    "print(\"Top_AAE: {:6.4f}\".format(top_aae))\n",
    "print(\"Estimate time:{:7.3f} seconds.\".format(endx-startx)) \n",
    "\n",
    "TF.Plot_hh_compare(temp,HH,\"SpaceSaving\")\n",
    "\n",
    "\n",
    "# result to csv\n",
    "path=\"..\\\\result\\\\SS\\\\\"+dataset+\"\\\\HH\\\\Top_\"+str(TC.size)\n",
    "filename='SS_HH'+'_'+str(len(tp_set))+'.csv'\n",
    "df=pd.DataFrame(Top_dict.items(),columns=['Element','Count'])\n",
    "df['Element'] = df['Element'].astype(str)\n",
    "df.to_csv(os.path.join(path,filename),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import Tools.Config as TC\n",
    "import Tools.Func as TF\n",
    "import Node.DS as DS\n",
    "import time\n",
    "import os\n",
    "\n",
    "from pympler import asizeof\n",
    "import re\n",
    "import pandas as pd\n",
    "dataset='webdocs'\n",
    "total_count=0\n",
    "heavy_ratio=1/5000\n",
    "\n",
    "# =============================dataset path and file=============================\n",
    "filepath=r\"..\\..\\dataset\\webdocs\"\n",
    "pattern='out_.*'\n",
    "r=re.compile(pattern)\n",
    "filelist=list(filter(r.match,os.listdir(filepath)))    \n",
    "    # dataset file list\n",
    "\n",
    "gr_file_name='webdocs_00_ground_truth.csv'        \n",
    "gr_path=os.path.join(filepath,gr_file_name)\n",
    "    # ground truth path\n",
    "    \n",
    "# =============================Initialize=============================\n",
    "w=0\n",
    "d=0\n",
    "size=4500\n",
    "topk=4500\n",
    "TC.Set_default(w,d,size,topk)\n",
    "    # set width, depth, size of Sk, random seed of hash\n",
    "    # Config.width, Config.depth\n",
    "Top_dict=dict()\n",
    "    # Top_dict[x]=[count_x,error_x]\n",
    "\n",
    "#item_count=100\n",
    "# =============================Stream processing=============================\n",
    "start=time.time()\n",
    "for datafile in filelist[:1]:\n",
    "    src_data=os.path.join(filepath,datafile)\n",
    "    with open(src_data,'r') as file:\n",
    "        while True:\n",
    "            e=file.readline().strip('\\n')\n",
    "            if not e:\n",
    "                print(\"EOF\")\n",
    "                break\n",
    "            else:\n",
    "                total_count+=1\n",
    "                item=DS.ssNode(e,1)\n",
    "                #item_count-=1\n",
    "                # print(\"read {}th element: {}\".format(item_count,element))\n",
    "                if Top_dict.get(item.ID):\n",
    "                    # e in Top\n",
    "                    Top_dict[item.ID][0]+=item.count\n",
    "                else:\n",
    "                    if len(Top_dict)<TC.size:\n",
    "                        Top_dict[item.ID]=[item.count,item.error]\n",
    "                    else:\n",
    "                        min_ele = min(Top_dict, key=lambda x: Top_dict.get(x)[0])\n",
    "                            # find e_min\n",
    "                        Top_dict[item.ID]=[Top_dict[min_ele][0]+item.count,Top_dict[min_ele][0]]\n",
    "                            # update c_min,c_error\n",
    "                        Top_dict.pop(min_ele)\n",
    "                            # pop old min out\n",
    "end=time.time()\n",
    "\n",
    "# =============================Print and Plot result=============================\n",
    "Top_dict=dict(sorted(Top_dict.items(), key=lambda item: item[1],reverse=True))\n",
    "print(\"Top-{}\".format(TC.size))\n",
    "print(\"Execution time:{:8.3f} seconds.\".format(end-start))\n",
    "\n",
    "# =============================heavy hitter from result=============================\n",
    "HH=dict()\n",
    "for item in Top_dict:\n",
    "    if Top_dict[item][0]>total_count*heavy_ratio:\n",
    "        if (Top_dict[item][0]-Top_dict[item][1])>=total_count*heavy_ratio:\n",
    "            HH[item]=Top_dict[item][0]\n",
    "HH=dict(sorted(HH.items(), key=lambda item: item[1],reverse=True)) \n",
    "\n",
    "# memory usage\n",
    "print(\"Top_dict with {} kbytes.\".format(asizeof.asizeof(Top_dict)/1024))\n",
    "# =============================heavy hitter from ground truth=============================\n",
    "\n",
    "# read ground truth\n",
    "import pandas as pd\n",
    "df=pd.read_csv(gr_path)\n",
    "df['Element']=df['Element'].astype('str')\n",
    "temp=df[df['Count']>=int(total_count*heavy_ratio)]\n",
    "\n",
    "gr_set=set(temp['Element'])\n",
    "result=set(HH.keys())\n",
    "tp_set=gr_set & result\n",
    "\n",
    "print(\"\\nFor copy\")\n",
    "print(\"Top-{}\".format(TC.size))\n",
    "print(\"Top_dict with {} kbytes.\".format(asizeof.asizeof(Top_dict)/1024))\n",
    "print(\"Find {:.3f} of Heavy Hitters\".format(len(tp_set)/len(gr_set)))\n",
    "print(\"Execution time:{:8.3f} seconds.\".format(end-start))\n",
    "\n",
    "# Count ARE/AAE in Top\n",
    "startx=time.time()\n",
    "top_are,top_aae=TF.Get_ARE_AAE(gr_path,HH,tp_set)\n",
    "print(\"Find:{}\".format(len(tp_set)))\n",
    "#print(\"{} item found in SS[{}] compare with true Top-{}\".format(len(tp_set),Config.size,topk))\n",
    "endx=time.time()\n",
    "\n",
    "print(\"Top_ARE: {:6.4f}\".format(top_are))\n",
    "print(\"Top_AAE: {:6.4f}\".format(top_aae))\n",
    "print(\"Estimate time:{:7.3f} seconds.\".format(endx-startx)) \n",
    "\n",
    "TF.Plot_hh_compare(temp,HH,\"SpaceSaving\")\n",
    "\n",
    "# result to csv\n",
    "path=\"..\\\\result\\\\SS\\\\\"+dataset+\"\\\\HH\\\\Top_\"+str(TC.size)\n",
    "filename='SS_HH'+'_'+str(len(tp_set))+'.csv'\n",
    "df=pd.DataFrame(Top_dict.items(),columns=['Element','Count'])\n",
    "df['Element'] = df['Element'].astype(str)\n",
    "df.to_csv(os.path.join(path,filename),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import Tools.Config as TC\n",
    "import Tools.Func as TF\n",
    "import Node.DS as DS\n",
    "import time\n",
    "import os\n",
    "\n",
    "from pympler import asizeof\n",
    "import re\n",
    "import pandas as pd\n",
    "dataset='webdocs'\n",
    "total_count=0\n",
    "heavy_ratio=1/5000\n",
    "\n",
    "# =============================dataset path and file=============================\n",
    "filepath=r\"..\\..\\dataset\\webdocs\"\n",
    "pattern='out_.*'\n",
    "r=re.compile(pattern)\n",
    "filelist=list(filter(r.match,os.listdir(filepath)))    \n",
    "    # dataset file list\n",
    "\n",
    "gr_file_name='webdocs_00_ground_truth.csv'        \n",
    "gr_path=os.path.join(filepath,gr_file_name)\n",
    "    # ground truth path\n",
    "    \n",
    "# =============================Initialize=============================\n",
    "w=0\n",
    "d=0\n",
    "size=4000\n",
    "topk=4000\n",
    "TC.Set_default(w,d,size,topk)\n",
    "    # set width, depth, size of Sk, random seed of hash\n",
    "    # Config.width, Config.depth\n",
    "Top_dict=dict()\n",
    "    # Top_dict[x]=[count_x,error_x]\n",
    "\n",
    "#item_count=100\n",
    "# =============================Stream processing=============================\n",
    "start=time.time()\n",
    "for datafile in filelist[:1]:\n",
    "    src_data=os.path.join(filepath,datafile)\n",
    "    with open(src_data,'r') as file:\n",
    "        while True:\n",
    "            e=file.readline().strip('\\n')\n",
    "            if not e:\n",
    "                print(\"EOF\")\n",
    "                break\n",
    "            else:\n",
    "                total_count+=1\n",
    "                item=DS.ssNode(e,1)\n",
    "                #item_count-=1\n",
    "                # print(\"read {}th element: {}\".format(item_count,element))\n",
    "                if Top_dict.get(item.ID):\n",
    "                    # e in Top\n",
    "                    Top_dict[item.ID][0]+=item.count\n",
    "                else:\n",
    "                    if len(Top_dict)<TC.size:\n",
    "                        Top_dict[item.ID]=[item.count,item.error]\n",
    "                    else:\n",
    "                        min_ele = min(Top_dict, key=lambda x: Top_dict.get(x)[0])\n",
    "                            # find e_min\n",
    "                        Top_dict[item.ID]=[Top_dict[min_ele][0]+item.count,Top_dict[min_ele][0]]\n",
    "                            # update c_min,c_error\n",
    "                        Top_dict.pop(min_ele)\n",
    "                            # pop old min out\n",
    "end=time.time()\n",
    "\n",
    "# =============================Print and Plot result=============================\n",
    "Top_dict=dict(sorted(Top_dict.items(), key=lambda item: item[1],reverse=True))\n",
    "print(\"Top-{}\".format(TC.size))\n",
    "print(\"Execution time:{:8.3f} seconds.\".format(end-start))\n",
    "\n",
    "# =============================heavy hitter from result=============================\n",
    "HH=dict()\n",
    "for item in Top_dict:\n",
    "    if Top_dict[item][0]>total_count*heavy_ratio:\n",
    "        if (Top_dict[item][0]-Top_dict[item][1])>=total_count*heavy_ratio:\n",
    "            HH[item]=Top_dict[item][0]\n",
    "HH=dict(sorted(HH.items(), key=lambda item: item[1],reverse=True)) \n",
    "\n",
    "# memory usage\n",
    "print(\"Top_dict with {} kbytes.\".format(asizeof.asizeof(Top_dict)/1024))\n",
    "# =============================heavy hitter from ground truth=============================\n",
    "\n",
    "# read ground truth\n",
    "import pandas as pd\n",
    "df=pd.read_csv(gr_path)\n",
    "df['Element']=df['Element'].astype('str')\n",
    "temp=df[df['Count']>=int(total_count*heavy_ratio)]\n",
    "\n",
    "gr_set=set(temp['Element'])\n",
    "result=set(HH.keys())\n",
    "tp_set=gr_set & result\n",
    "\n",
    "print(\"\\nFor copy\")\n",
    "print(\"Top-{}\".format(TC.size))\n",
    "print(\"Top_dict with {} kbytes.\".format(asizeof.asizeof(Top_dict)/1024))\n",
    "print(\"Find {:.3f} of Heavy Hitters\".format(len(tp_set)/len(gr_set)))\n",
    "print(\"Execution time:{:8.3f} seconds.\".format(end-start))\n",
    "\n",
    "# Count ARE/AAE in Top\n",
    "startx=time.time()\n",
    "top_are,top_aae=TF.Get_ARE_AAE(gr_path,HH,tp_set)\n",
    "print(\"Find:{}\".format(len(tp_set)))\n",
    "#print(\"{} item found in SS[{}] compare with true Top-{}\".format(len(tp_set),Config.size,topk))\n",
    "endx=time.time()\n",
    "\n",
    "print(\"Top_ARE: {:6.4f}\".format(top_are))\n",
    "print(\"Top_AAE: {:6.4f}\".format(top_aae))\n",
    "print(\"Estimate time:{:7.3f} seconds.\".format(endx-startx)) \n",
    "\n",
    "TF.Plot_hh_compare(temp,HH,\"SpaceSaving\")\n",
    "\n",
    "# result to csv\n",
    "path=\"..\\\\result\\\\SS\\\\\"+dataset+\"\\\\HH\\\\Top_\"+str(TC.size)\n",
    "filename='SS_HH'+'_'+str(len(tp_set))+'.csv'\n",
    "df=pd.DataFrame(Top_dict.items(),columns=['Element','Count'])\n",
    "df['Element'] = df['Element'].astype(str)\n",
    "df.to_csv(os.path.join(path,filename),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import Tools.Config as TC\n",
    "import Tools.Func as TF\n",
    "import Node.DS as DS\n",
    "import time\n",
    "import os\n",
    "\n",
    "from pympler import asizeof\n",
    "import re\n",
    "import pandas as pd\n",
    "dataset='webdocs'\n",
    "total_count=0\n",
    "heavy_ratio=1/5000\n",
    "\n",
    "# =============================dataset path and file=============================\n",
    "filepath=r\"..\\..\\dataset\\webdocs\"\n",
    "pattern='out_.*'\n",
    "r=re.compile(pattern)\n",
    "filelist=list(filter(r.match,os.listdir(filepath)))    \n",
    "    # dataset file list\n",
    "\n",
    "gr_file_name='webdocs_00_ground_truth.csv'        \n",
    "gr_path=os.path.join(filepath,gr_file_name)\n",
    "    # ground truth path\n",
    "    \n",
    "# =============================Initialize=============================\n",
    "w=0\n",
    "d=0\n",
    "size=3500\n",
    "topk=3500\n",
    "TC.Set_default(w,d,size,topk)\n",
    "    # set width, depth, size of Sk, random seed of hash\n",
    "    # Config.width, Config.depth\n",
    "Top_dict=dict()\n",
    "    # Top_dict[x]=[count_x,error_x]\n",
    "\n",
    "#item_count=100\n",
    "# =============================Stream processing=============================\n",
    "start=time.time()\n",
    "for datafile in filelist[:1]:\n",
    "    src_data=os.path.join(filepath,datafile)\n",
    "    with open(src_data,'r') as file:\n",
    "        while True:\n",
    "            e=file.readline().strip('\\n')\n",
    "            if not e:\n",
    "                print(\"EOF\")\n",
    "                break\n",
    "            else:\n",
    "                total_count+=1\n",
    "                item=DS.ssNode(e,1)\n",
    "                #item_count-=1\n",
    "                # print(\"read {}th element: {}\".format(item_count,element))\n",
    "                if Top_dict.get(item.ID):\n",
    "                    # e in Top\n",
    "                    Top_dict[item.ID][0]+=item.count\n",
    "                else:\n",
    "                    if len(Top_dict)<TC.size:\n",
    "                        Top_dict[item.ID]=[item.count,item.error]\n",
    "                    else:\n",
    "                        min_ele = min(Top_dict, key=lambda x: Top_dict.get(x)[0])\n",
    "                            # find e_min\n",
    "                        Top_dict[item.ID]=[Top_dict[min_ele][0]+item.count,Top_dict[min_ele][0]]\n",
    "                            # update c_min,c_error\n",
    "                        Top_dict.pop(min_ele)\n",
    "                            # pop old min out\n",
    "end=time.time()\n",
    "\n",
    "# =============================Print and Plot result=============================\n",
    "Top_dict=dict(sorted(Top_dict.items(), key=lambda item: item[1],reverse=True))\n",
    "print(\"Top-{}\".format(TC.size))\n",
    "print(\"Execution time:{:8.3f} seconds.\".format(end-start))\n",
    "\n",
    "# =============================heavy hitter from result=============================\n",
    "HH=dict()\n",
    "for item in Top_dict:\n",
    "    if Top_dict[item][0]>total_count*heavy_ratio:\n",
    "        if (Top_dict[item][0]-Top_dict[item][1])>=total_count*heavy_ratio:\n",
    "            HH[item]=Top_dict[item][0]\n",
    "HH=dict(sorted(HH.items(), key=lambda item: item[1],reverse=True)) \n",
    "\n",
    "# memory usage\n",
    "print(\"Top_dict with {} kbytes.\".format(asizeof.asizeof(Top_dict)/1024))\n",
    "# =============================heavy hitter from ground truth=============================\n",
    "\n",
    "# read ground truth\n",
    "import pandas as pd\n",
    "df=pd.read_csv(gr_path)\n",
    "df['Element']=df['Element'].astype('str')\n",
    "temp=df[df['Count']>=int(total_count*heavy_ratio)]\n",
    "\n",
    "gr_set=set(temp['Element'])\n",
    "result=set(HH.keys())\n",
    "tp_set=gr_set & result\n",
    "\n",
    "print(\"\\nFor copy\")\n",
    "print(\"Top-{}\".format(TC.size))\n",
    "print(\"Top_dict with {} kbytes.\".format(asizeof.asizeof(Top_dict)/1024))\n",
    "print(\"Find {:.3f} of Heavy Hitters\".format(len(tp_set)/len(gr_set)))\n",
    "print(\"Execution time:{:8.3f} seconds.\".format(end-start))\n",
    "\n",
    "# Count ARE/AAE in Top\n",
    "startx=time.time()\n",
    "top_are,top_aae=TF.Get_ARE_AAE(gr_path,HH,tp_set)\n",
    "print(\"Find:{}\".format(len(tp_set)))\n",
    "#print(\"{} item found in SS[{}] compare with true Top-{}\".format(len(tp_set),Config.size,topk))\n",
    "endx=time.time()\n",
    "\n",
    "print(\"Top_ARE: {:6.4f}\".format(top_are))\n",
    "print(\"Top_AAE: {:6.4f}\".format(top_aae))\n",
    "print(\"Estimate time:{:7.3f} seconds.\".format(endx-startx)) \n",
    "\n",
    "TF.Plot_hh_compare(temp,HH,\"SpaceSaving\")\n",
    "\n",
    "# result to csv\n",
    "path=\"..\\\\result\\\\SS\\\\\"+dataset+\"\\\\HH\\\\Top_\"+str(TC.size)\n",
    "filename='SS_HH'+'_'+str(len(tp_set))+'.csv'\n",
    "df=pd.DataFrame(Top_dict.items(),columns=['Element','Count'])\n",
    "df['Element'] = df['Element'].astype(str)\n",
    "df.to_csv(os.path.join(path,filename),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import Tools.Config as TC\n",
    "import Tools.Func as TF\n",
    "import Node.DS as DS\n",
    "import time\n",
    "import os\n",
    "\n",
    "from pympler import asizeof\n",
    "import re\n",
    "import pandas as pd\n",
    "dataset='webdocs'\n",
    "total_count=0\n",
    "heavy_ratio=1/5000\n",
    "\n",
    "# =============================dataset path and file=============================\n",
    "filepath=r\"..\\..\\dataset\\webdocs\"\n",
    "pattern='out_.*'\n",
    "r=re.compile(pattern)\n",
    "filelist=list(filter(r.match,os.listdir(filepath)))    \n",
    "    # dataset file list\n",
    "\n",
    "gr_file_name='webdocs_00_ground_truth.csv'        \n",
    "gr_path=os.path.join(filepath,gr_file_name)\n",
    "    # ground truth path\n",
    "    \n",
    "# =============================Initialize=============================\n",
    "w=0\n",
    "d=0\n",
    "size=3000\n",
    "topk=3000\n",
    "TC.Set_default(w,d,size,topk)\n",
    "    # set width, depth, size of Sk, random seed of hash\n",
    "    # Config.width, Config.depth\n",
    "Top_dict=dict()\n",
    "    # Top_dict[x]=[count_x,error_x]\n",
    "\n",
    "#item_count=100\n",
    "# =============================Stream processing=============================\n",
    "start=time.time()\n",
    "for datafile in filelist[:1]:\n",
    "    src_data=os.path.join(filepath,datafile)\n",
    "    with open(src_data,'r') as file:\n",
    "        while True:\n",
    "            e=file.readline().strip('\\n')\n",
    "            if not e:\n",
    "                print(\"EOF\")\n",
    "                break\n",
    "            else:\n",
    "                total_count+=1\n",
    "                item=DS.ssNode(e,1)\n",
    "                #item_count-=1\n",
    "                # print(\"read {}th element: {}\".format(item_count,element))\n",
    "                if Top_dict.get(item.ID):\n",
    "                    # e in Top\n",
    "                    Top_dict[item.ID][0]+=item.count\n",
    "                else:\n",
    "                    if len(Top_dict)<TC.size:\n",
    "                        Top_dict[item.ID]=[item.count,item.error]\n",
    "                    else:\n",
    "                        min_ele = min(Top_dict, key=lambda x: Top_dict.get(x)[0])\n",
    "                            # find e_min\n",
    "                        Top_dict[item.ID]=[Top_dict[min_ele][0]+item.count,Top_dict[min_ele][0]]\n",
    "                            # update c_min,c_error\n",
    "                        Top_dict.pop(min_ele)\n",
    "                            # pop old min out\n",
    "end=time.time()\n",
    "\n",
    "# =============================Print and Plot result=============================\n",
    "Top_dict=dict(sorted(Top_dict.items(), key=lambda item: item[1],reverse=True))\n",
    "print(\"Top-{}\".format(TC.size))\n",
    "print(\"Execution time:{:8.3f} seconds.\".format(end-start))\n",
    "\n",
    "# =============================heavy hitter from result=============================\n",
    "HH=dict()\n",
    "for item in Top_dict:\n",
    "    if Top_dict[item][0]>total_count*heavy_ratio:\n",
    "        if (Top_dict[item][0]-Top_dict[item][1])>=total_count*heavy_ratio:\n",
    "            HH[item]=Top_dict[item][0]\n",
    "HH=dict(sorted(HH.items(), key=lambda item: item[1],reverse=True)) \n",
    "\n",
    "# memory usage\n",
    "print(\"Top_dict with {} kbytes.\".format(asizeof.asizeof(Top_dict)/1024))\n",
    "# =============================heavy hitter from ground truth=============================\n",
    "\n",
    "# read ground truth\n",
    "import pandas as pd\n",
    "df=pd.read_csv(gr_path)\n",
    "df['Element']=df['Element'].astype('str')\n",
    "temp=df[df['Count']>=int(total_count*heavy_ratio)]\n",
    "\n",
    "gr_set=set(temp['Element'])\n",
    "result=set(HH.keys())\n",
    "tp_set=gr_set & result\n",
    "\n",
    "print(\"\\nFor copy\")\n",
    "print(\"Top-{}\".format(TC.size))\n",
    "print(\"Top_dict with {} kbytes.\".format(asizeof.asizeof(Top_dict)/1024))\n",
    "print(\"Find {:.3f} of Heavy Hitters\".format(len(tp_set)/len(gr_set)))\n",
    "print(\"Execution time:{:8.3f} seconds.\".format(end-start))\n",
    "\n",
    "# Count ARE/AAE in Top\n",
    "startx=time.time()\n",
    "top_are,top_aae=TF.Get_ARE_AAE(gr_path,HH,tp_set)\n",
    "print(\"Find:{}\".format(len(tp_set)))\n",
    "#print(\"{} item found in SS[{}] compare with true Top-{}\".format(len(tp_set),Config.size,topk))\n",
    "endx=time.time()\n",
    "\n",
    "print(\"Top_ARE: {:6.4f}\".format(top_are))\n",
    "print(\"Top_AAE: {:6.4f}\".format(top_aae))\n",
    "print(\"Estimate time:{:7.3f} seconds.\".format(endx-startx)) \n",
    "\n",
    "TF.Plot_hh_compare(temp,HH,\"SpaceSaving\")\n",
    "\n",
    "# result to csv\n",
    "path=\"..\\\\result\\\\SS\\\\\"+dataset+\"\\\\HH\\\\Top_\"+str(TC.size)\n",
    "filename='SS_HH'+'_'+str(len(tp_set))+'.csv'\n",
    "df=pd.DataFrame(Top_dict.items(),columns=['Element','Count'])\n",
    "df['Element'] = df['Element'].astype(str)\n",
    "df.to_csv(os.path.join(path,filename),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import Tools.Config as TC\n",
    "import Tools.Func as TF\n",
    "import Node.DS as DS\n",
    "import time\n",
    "import os\n",
    "\n",
    "from pympler import asizeof\n",
    "import re\n",
    "import pandas as pd\n",
    "dataset='webdocs'\n",
    "total_count=0\n",
    "heavy_ratio=1/5000\n",
    "\n",
    "# =============================dataset path and file=============================\n",
    "filepath=r\"..\\..\\dataset\\webdocs\"\n",
    "pattern='out_.*'\n",
    "r=re.compile(pattern)\n",
    "filelist=list(filter(r.match,os.listdir(filepath)))    \n",
    "    # dataset file list\n",
    "\n",
    "gr_file_name='webdocs_00_ground_truth.csv'        \n",
    "gr_path=os.path.join(filepath,gr_file_name)\n",
    "    # ground truth path\n",
    "    \n",
    "# =============================Initialize=============================\n",
    "w=0\n",
    "d=0\n",
    "size=2500\n",
    "topk=2500\n",
    "TC.Set_default(w,d,size,topk)\n",
    "    # set width, depth, size of Sk, random seed of hash\n",
    "    # Config.width, Config.depth\n",
    "Top_dict=dict()\n",
    "    # Top_dict[x]=[count_x,error_x]\n",
    "\n",
    "#item_count=100\n",
    "# =============================Stream processing=============================\n",
    "start=time.time()\n",
    "for datafile in filelist[:1]:\n",
    "    src_data=os.path.join(filepath,datafile)\n",
    "    with open(src_data,'r') as file:\n",
    "        while True:\n",
    "            e=file.readline().strip('\\n')\n",
    "            if not e:\n",
    "                print(\"EOF\")\n",
    "                break\n",
    "            else:\n",
    "                total_count+=1\n",
    "                item=DS.ssNode(e,1)\n",
    "                #item_count-=1\n",
    "                # print(\"read {}th element: {}\".format(item_count,element))\n",
    "                if Top_dict.get(item.ID):\n",
    "                    # e in Top\n",
    "                    Top_dict[item.ID][0]+=item.count\n",
    "                else:\n",
    "                    if len(Top_dict)<TC.size:\n",
    "                        Top_dict[item.ID]=[item.count,item.error]\n",
    "                    else:\n",
    "                        min_ele = min(Top_dict, key=lambda x: Top_dict.get(x)[0])\n",
    "                            # find e_min\n",
    "                        Top_dict[item.ID]=[Top_dict[min_ele][0]+item.count,Top_dict[min_ele][0]]\n",
    "                            # update c_min,c_error\n",
    "                        Top_dict.pop(min_ele)\n",
    "                            # pop old min out\n",
    "end=time.time()\n",
    "\n",
    "# =============================Print and Plot result=============================\n",
    "Top_dict=dict(sorted(Top_dict.items(), key=lambda item: item[1],reverse=True))\n",
    "print(\"Top-{}\".format(TC.size))\n",
    "print(\"Execution time:{:8.3f} seconds.\".format(end-start))\n",
    "\n",
    "# =============================heavy hitter from result=============================\n",
    "HH=dict()\n",
    "for item in Top_dict:\n",
    "    if Top_dict[item][0]>total_count*heavy_ratio:\n",
    "        if (Top_dict[item][0]-Top_dict[item][1])>=total_count*heavy_ratio:\n",
    "            HH[item]=Top_dict[item][0]\n",
    "HH=dict(sorted(HH.items(), key=lambda item: item[1],reverse=True)) \n",
    "\n",
    "# memory usage\n",
    "print(\"Top_dict with {} kbytes.\".format(asizeof.asizeof(Top_dict)/1024))\n",
    "# =============================heavy hitter from ground truth=============================\n",
    "\n",
    "# read ground truth\n",
    "import pandas as pd\n",
    "df=pd.read_csv(gr_path)\n",
    "df['Element']=df['Element'].astype('str')\n",
    "temp=df[df['Count']>=int(total_count*heavy_ratio)]\n",
    "\n",
    "gr_set=set(temp['Element'])\n",
    "result=set(HH.keys())\n",
    "tp_set=gr_set & result\n",
    "\n",
    "print(\"\\nFor copy\")\n",
    "print(\"Top-{}\".format(TC.size))\n",
    "print(\"Top_dict with {} kbytes.\".format(asizeof.asizeof(Top_dict)/1024))\n",
    "print(\"Find {:.3f} of Heavy Hitters\".format(len(tp_set)/len(gr_set)))\n",
    "print(\"Execution time:{:8.3f} seconds.\".format(end-start))\n",
    "\n",
    "# Count ARE/AAE in Top\n",
    "startx=time.time()\n",
    "top_are,top_aae=TF.Get_ARE_AAE(gr_path,HH,tp_set)\n",
    "print(\"Find:{}\".format(len(tp_set)))\n",
    "#print(\"{} item found in SS[{}] compare with true Top-{}\".format(len(tp_set),Config.size,topk))\n",
    "endx=time.time()\n",
    "\n",
    "print(\"Top_ARE: {:6.4f}\".format(top_are))\n",
    "print(\"Top_AAE: {:6.4f}\".format(top_aae))\n",
    "print(\"Estimate time:{:7.3f} seconds.\".format(endx-startx)) \n",
    "\n",
    "TF.Plot_hh_compare(temp,HH,\"SpaceSaving\")\n",
    "\n",
    "# result to csv\n",
    "path=\"..\\\\result\\\\SS\\\\\"+dataset+\"\\\\HH\\\\Top_\"+str(TC.size)\n",
    "filename='SS_HH'+'_'+str(len(tp_set))+'.csv'\n",
    "df=pd.DataFrame(Top_dict.items(),columns=['Element','Count'])\n",
    "df['Element'] = df['Element'].astype(str)\n",
    "df.to_csv(os.path.join(path,filename),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import Tools.Config as TC\n",
    "import Tools.Func as TF\n",
    "import Node.DS as DS\n",
    "import time\n",
    "import os\n",
    "\n",
    "from pympler import asizeof\n",
    "import re\n",
    "import pandas as pd\n",
    "dataset='webdocs'\n",
    "total_count=0\n",
    "heavy_ratio=1/5000\n",
    "\n",
    "# =============================dataset path and file=============================\n",
    "filepath=r\"..\\..\\dataset\\webdocs\"\n",
    "pattern='out_.*'\n",
    "r=re.compile(pattern)\n",
    "filelist=list(filter(r.match,os.listdir(filepath)))    \n",
    "    # dataset file list\n",
    "\n",
    "gr_file_name='webdocs_00_ground_truth.csv'        \n",
    "gr_path=os.path.join(filepath,gr_file_name)\n",
    "    # ground truth path\n",
    "    \n",
    "# =============================Initialize=============================\n",
    "w=0\n",
    "d=0\n",
    "size=2000\n",
    "topk=2000\n",
    "TC.Set_default(w,d,size,topk)\n",
    "    # set width, depth, size of Sk, random seed of hash\n",
    "    # Config.width, Config.depth\n",
    "Top_dict=dict()\n",
    "    # Top_dict[x]=[count_x,error_x]\n",
    "\n",
    "#item_count=100\n",
    "# =============================Stream processing=============================\n",
    "start=time.time()\n",
    "for datafile in filelist[:1]:\n",
    "    src_data=os.path.join(filepath,datafile)\n",
    "    with open(src_data,'r') as file:\n",
    "        while True:\n",
    "            e=file.readline().strip('\\n')\n",
    "            if not e:\n",
    "                print(\"EOF\")\n",
    "                break\n",
    "            else:\n",
    "                total_count+=1\n",
    "                item=DS.ssNode(e,1)\n",
    "                #item_count-=1\n",
    "                # print(\"read {}th element: {}\".format(item_count,element))\n",
    "                if Top_dict.get(item.ID):\n",
    "                    # e in Top\n",
    "                    Top_dict[item.ID][0]+=item.count\n",
    "                else:\n",
    "                    if len(Top_dict)<TC.size:\n",
    "                        Top_dict[item.ID]=[item.count,item.error]\n",
    "                    else:\n",
    "                        min_ele = min(Top_dict, key=lambda x: Top_dict.get(x)[0])\n",
    "                            # find e_min\n",
    "                        Top_dict[item.ID]=[Top_dict[min_ele][0]+item.count,Top_dict[min_ele][0]]\n",
    "                            # update c_min,c_error\n",
    "                        Top_dict.pop(min_ele)\n",
    "                            # pop old min out\n",
    "end=time.time()\n",
    "\n",
    "# =============================Print and Plot result=============================\n",
    "Top_dict=dict(sorted(Top_dict.items(), key=lambda item: item[1],reverse=True))\n",
    "print(\"Top-{}\".format(TC.size))\n",
    "print(\"Execution time:{:8.3f} seconds.\".format(end-start))\n",
    "\n",
    "# =============================heavy hitter from result=============================\n",
    "HH=dict()\n",
    "for item in Top_dict:\n",
    "    if Top_dict[item][0]>total_count*heavy_ratio:\n",
    "        if (Top_dict[item][0]-Top_dict[item][1])>=total_count*heavy_ratio:\n",
    "            HH[item]=Top_dict[item][0]\n",
    "HH=dict(sorted(HH.items(), key=lambda item: item[1],reverse=True)) \n",
    "\n",
    "# memory usage\n",
    "print(\"Top_dict with {} kbytes.\".format(asizeof.asizeof(Top_dict)/1024))\n",
    "# =============================heavy hitter from ground truth=============================\n",
    "\n",
    "# read ground truth\n",
    "import pandas as pd\n",
    "df=pd.read_csv(gr_path)\n",
    "df['Element']=df['Element'].astype('str')\n",
    "temp=df[df['Count']>=int(total_count*heavy_ratio)]\n",
    "\n",
    "gr_set=set(temp['Element'])\n",
    "result=set(HH.keys())\n",
    "tp_set=gr_set & result\n",
    "\n",
    "print(\"\\nFor copy\")\n",
    "print(\"Top-{}\".format(TC.size))\n",
    "print(\"Top_dict with {} kbytes.\".format(asizeof.asizeof(Top_dict)/1024))\n",
    "print(\"Find {:.3f} of Heavy Hitters\".format(len(tp_set)/len(gr_set)))\n",
    "print(\"Execution time:{:8.3f} seconds.\".format(end-start))\n",
    "\n",
    "# Count ARE/AAE in Top\n",
    "startx=time.time()\n",
    "top_are,top_aae=TF.Get_ARE_AAE(gr_path,HH,tp_set)\n",
    "print(\"Find:{}\".format(len(tp_set)))\n",
    "#print(\"{} item found in SS[{}] compare with true Top-{}\".format(len(tp_set),Config.size,topk))\n",
    "endx=time.time()\n",
    "\n",
    "print(\"Top_ARE: {:6.4f}\".format(top_are))\n",
    "print(\"Top_AAE: {:6.4f}\".format(top_aae))\n",
    "print(\"Estimate time:{:7.3f} seconds.\".format(endx-startx)) \n",
    "\n",
    "TF.Plot_hh_compare(temp,HH,\"SpaceSaving\")\n",
    "\n",
    "# result to csv\n",
    "path=\"..\\\\result\\\\SS\\\\\"+dataset+\"\\\\HH\\\\Top_\"+str(TC.size)\n",
    "filename='SS_HH'+'_'+str(len(tp_set))+'.csv'\n",
    "df=pd.DataFrame(Top_dict.items(),columns=['Element','Count'])\n",
    "df['Element'] = df['Element'].astype(str)\n",
    "df.to_csv(os.path.join(path,filename),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x=np.zeros(2,dtype='int32')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incoming:1\n",
      "Top_dict:{}\n",
      "Incoming:2\n",
      "Top_dict:{'1': array([1, 0])}\n",
      "Incoming:3\n",
      "Top_dict:{'1': array([1, 0]), '2': array([1, 0])}\n",
      "Incoming:4\n",
      "Top_dict:{'1': array([1, 0]), '2': array([1, 0]), '3': array([1, 0])}\n",
      "Incoming:5\n",
      "Top_dict:{'1': array([1, 0]), '2': array([1, 0]), '3': array([1, 0]), '4': array([1, 0])}\n",
      "Incoming:6\n",
      "Top_dict:{'1': array([1, 0]), '2': array([1, 0]), '3': array([1, 0]), '4': array([1, 0]), '5': array([1, 0])}\n",
      "Incoming:7\n",
      "Top_dict:{'2': array([1, 0]), '3': array([1, 0]), '4': array([1, 0]), '5': array([1, 0]), '6': [2, 1]}\n",
      "Incoming:8\n",
      "Top_dict:{'3': array([1, 0]), '4': array([1, 0]), '5': array([1, 0]), '6': [2, 1], '7': [2, 1]}\n",
      "Incoming:9\n",
      "Top_dict:{'4': array([1, 0]), '5': array([1, 0]), '6': [2, 1], '7': [2, 1], '8': [2, 1]}\n",
      "Incoming:10\n",
      "Top_dict:{'5': array([1, 0]), '6': [2, 1], '7': [2, 1], '8': [2, 1], '9': [2, 1]}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import Tools.Config as TC\n",
    "import Tools.Func as TF\n",
    "import Node.DS as DS\n",
    "import time\n",
    "import os\n",
    "\n",
    "from pympler import asizeof\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset='webdocs'\n",
    "total_count=0\n",
    "heavy_ratio=1/5000\n",
    "\n",
    "# =============================dataset path and file=============================\n",
    "filepath=r\"..\\..\\dataset\\webdocs\"\n",
    "pattern='out_.*'\n",
    "r=re.compile(pattern)\n",
    "filelist=list(filter(r.match,os.listdir(filepath)))    \n",
    "    # dataset file list\n",
    "\n",
    "gr_file_name='webdocs_00_ground_truth.csv'        \n",
    "gr_path=os.path.join(filepath,gr_file_name)\n",
    "    # ground truth path\n",
    "    \n",
    "# =============================Initialize=============================\n",
    "w=0\n",
    "d=0\n",
    "size=5\n",
    "topk=5\n",
    "TC.Set_default(w,d,size,topk)\n",
    "    # set width, depth, size of Sk, random seed of hash\n",
    "    # Config.width, Config.depth\n",
    "Top_dict=dict()\n",
    "    # Top_dict[x]=[count_x,error_x]\n",
    "\n",
    "item_count=10\n",
    "\n",
    "# =============================Stream processing=============================\n",
    "start=time.time()\n",
    "for datafile in filelist[:1]:\n",
    "    src_data=os.path.join(filepath,datafile)\n",
    "    with open(src_data,'r') as file:\n",
    "        while item_count:\n",
    "            item=file.readline().strip('\\n')\n",
    "            if not item:\n",
    "                print(\"EOF\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Incoming:{}\".format(item))\n",
    "                print(\"Top_dict:{}\".format(Top_dict))\n",
    "                total_count+=1\n",
    "                item_count-=1\n",
    "                # print(\"read {}th element: {}\".format(item_count,element))\n",
    "                if Top_dict.get(item):\n",
    "                    # e in Top\n",
    "                    Top_dict[item][0]+=1\n",
    "                else:\n",
    "                    if len(Top_dict)<TC.size:\n",
    "                        Top_dict[item]=np.array((1,0),dtype='int32')\n",
    "                    else:\n",
    "                        min_ele = min(Top_dict, key=lambda x: Top_dict.get(x)[0])\n",
    "                            # find e_min\n",
    "                        Top_dict[item]=[Top_dict[min_ele][0]+1,Top_dict[min_ele][0]]\n",
    "                            # update c_min,c_error\n",
    "                        Top_dict.pop(min_ele)\n",
    "                            # pop old min out\n",
    "end=time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x=dict()\n",
    "x['a']=np.array((6,1),dtype='int32')\n",
    "x['b']=np.array((2,2),dtype='int32')\n",
    "x['c']=np.array((4,2),dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_ele = min(x, key=lambda a: x.get(a)[0])\n",
    "min_ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
